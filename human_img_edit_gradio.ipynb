{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473523df",
   "metadata": {},
   "source": [
    "### Example Jupyter Code for Generation Results with Gradio Demo\n",
    "\n",
    "```\n",
    "--------------------------------------------------------\n",
    "DisCo - Disentangled Control for Referring Human Dance Generation in Real World\n",
    "Licensed under The Apache-2.0 license License [see LICENSE for details]\n",
    "Tan Wang (TAN317@e.ntu.edu.sg)\n",
    "Work done during internship at Microsoft\n",
    "--------------------------------------------------------\n",
    "```\n",
    "\n",
    "Pls remember to change the path (model checkpoint, root dir, eval_save_filename, and so on) in the `manual_args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bea7bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbf0c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 18:01:01 <wutils_ldm.py:156> <module>] <utils.py>: Deep Learning Utils @ Chenfei Wu\u001b[0m\n",
      "/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_ENABLE: 0\n",
      "2024-09-26 18:01:02,193.193 2301975:common.py:1785 setup_yaml(): python 3 env\n",
      "[2024-09-26 18:01:02,424] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sahil/anaconda3/envs/disco1/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/scratch/sahil/anaconda3/envs/disco1/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "[2024-09-26 18:01:03 <wutils.py:132> <module>] <utils.py>: Deep Learning Utils @ Chenfei Wu\u001b[0m\n",
      "[2024-09-26 18:01:03 <wutils.py:132> <module>] <utils.py>: Deep Learning Utils @ Chenfei Wu\u001b[0m\n",
      "[2024-09-26 18:01:03 <__init__.py:71> BasicArgs] Detected unknown Node katz.\u001b[0m\n",
      "[2024-09-26 18:01:03 <__init__.py:71> BasicArgs] Detected unknown Node katz.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_ENABLE\"] = \"0\"\n",
    "\n",
    "from utils.wutils_ldm import *\n",
    "from agent import Agent_LDM, WarmupLinearLR, WarmupLinearConstantLR\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from config import BasicArgs\n",
    "from utils.lib import *\n",
    "from utils.args import parse_with_cf\n",
    "\n",
    "from utils.dist import dist_init\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '23658'\n",
    "os.environ[\"USE_LIBUV\"] = \"0\"\n",
    "\n",
    "# # initialize the process group\n",
    "# dist.init_process_group(\n",
    "#     backend=\"cuda:gloo\",\n",
    "#     rank=0,\n",
    "#     world_size=1,\n",
    "#     init_method=f\"tcp://localhost:23456\",\n",
    "# )\n",
    "\n",
    "from dataset.tsv_dataset import make_data_sampler, make_batch_data_sampler\n",
    "from finetune_sdm_yaml import get_loader_info, make_data_loader\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8735d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fcd501",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 18:01:03 <2307042429.py:73> <module>] Building models...\u001b[0m\n",
      "[2024-09-26 18:01:03 <2307042429.py:73> <module>] Building models...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed training ... presumbly debug with 1 GPU\n",
      "Using seed 42 for rank 0\n",
      "Using seed 42 for torch.cuda\n",
      "Loading pre-trained image_encoder from /scratch/namrata/3d_pose_gen/diffusers/sd-image-variations-diffusers/image_encoder\n",
      "Loading pre-trained vae from /scratch/namrata/3d_pose_gen/diffusers/sd-image-variations-diffusers/vae\n",
      "Loading pre-trained unet from /scratch/namrata/3d_pose_gen/diffusers/sd-image-variations-diffusers/unet\n",
      "use sd vae to init the controlnet condition embedding\n",
      "Args: {'root_dir': '/scratch/namrata/3d_pose_gen', 'cf': 'config/ref_attn_clip_combine_controlnet/app_demo_image_edit.py', 'pretrained_model': 'exp/tiktok_ft22_final/1999.pth/mp_rank_00_model_states.pt', 'pretrained_model_lora': None, 'pretrained_model_controlnet': None, 'debug': False, 'debug_seed': False, 'debug_dataloader': False, 'log_dir': '/scratch/namrata/3d_pose_gen/exp/tiktok_ft5', 'deepspeed': True, 'use_amp': False, 'seed': 42, 'fix_dist_seed': True, 'tiktok_data_root': 'keli/dataset/TikTok_dataset/', 'img_size': (256, 256), 'max_video_len': 1, 'debug_max_video_len': 1, 'conds': ['poses', 'masks'], 'gradient_checkpointing': True, 'find_unused_parameters': False, 'enable_xformers_memory_efficient_attention': True, 'trainable_modules': None, 'scale_factor': 0.18215, 'loss_target': 'noise', 'x0_steps': 200, 'pretrained_model_path': '/scratch/namrata/3d_pose_gen/diffusers/sd-image-variations-diffusers', 'num_workers': 4, 'node_split_sampler': False, 'gradient_accumulate_steps': 1, 'max_grad_norm': -1, 'learning_rate': 0.0002, 'decay': 0.001, 'warmup_ratio': 0.1, 'max_train_samples': None, 'debug_max_train_samples': 100, 'drop_text': 1.0, 'local_train_batch_size': 32, 'epochs': 20, 'eval_step': 500.0, 'save_step': 500.0, 'do_train': False, 'train_yaml': './blob_dir/debug_output/video_sythesis/dataset/composite/train_webvid10m_a_54.yaml', 'resume': False, 'null_caption': False, 'refer_sdvae': True, 'controlnet_conditioning_scale_cond': 1.0, 'controlnet_conditioning_scale_ref': 1.0, 'nframes': 8, 'frame_interval': 1, 'eval_sample_interval': 1, 'train_sample_interval': 1, 'unet_unfreeze_type': 'all', 'controlnet_attn': False, 'use_cfg': False, 'refer_clip_preprocess': False, 'refer_clip_proj': False, 'ref_null_caption': False, 'combine_clip_local': True, 'combine_use_mask': True, 'drop_ref': 0.0, 'my_adapter': False, 'pos_resize_img': False, 'fg_variation': 0.0, 'strong_aug_stage2': False, 'strong_rand_stage2': False, 'strong_aug_stage1': False, 'stage1_pretrain_path': None, 'stage2_only_pose': False, 'constant_lr': False, 'SD2_not_add_image_emb_noise': False, 'val_yaml': './blob_dir/debug_output/video_sythesis/dataset/composite/val_webvid10m_a.yaml', 'max_eval_samples': None, 'debug_max_eval_samples': 20, 'pose_normalize': False, 'normalize_by_1st_frm': False, 'local_eval_batch_size': 32, 'eval_visu': True, 'eval_visu_trainsample': False, 'eval_visu_imagefolder': False, 'eval_visu_changepose': False, 'eval_visu_changefore': False, 'eval_save_filename': 'try', 'eval_before_train': True, 'eval_scheduler': 'ddim', 'eval_enc_dec_only': False, 'num_inf_videos_per_prompt': 1, 'num_inference_steps': 50, 'guidance_scale': 3.0, 'stepwise_sample_depth': -1, 'interpolation': None, 'interpolate_mode': None, 'visu_save': False, 'freeze_pose': False, 'freeze_background': False, 'ft_img_num': 0, 'ft_one_ref_image': True, 'ft_iters': None, 's1': 1.0, 's2': 1.0, 'ft_idx': None, 'ref_mode': 'first', '__module__': 'mymodule', 'task_name': 'ref_attn_clip_combine_controlnet', 'method_name': 'app_demo_image_edit', 'dataset_cf': 'dataset/app_demo_human_image_edit_singleinput.py', 'img_full_size': (256, 256), 'fps': 5, 'data_dir': './blob_dir/data/mtp_vlp_ray/debug/debug_pretrain', 'debug_train_yaml': './blob_dir/data/mtp_vlp_ray/debug/debug_pretrain/composite/train_webvid2.5m_2.yaml', 'debug_val_yaml': './blob_dir/data/mtp_vlp_ray/debug/debug_pretrain/composite/val_webvid2.5m.yaml', 'web_data_root': '/datadrive_d/wangtan/azure_storage/vigstandard_data/linjli/debug_output/video_sythesis/dataset/Lindsey_0504_youtube/frames/single_person', 'sd15_path': '/scratch/namrata/3d_pose_gen/diffusers/stable-diffusion-v1-5-2', 'freeze_unet': True, 'num_inf_images_per_prompt': 1, '__doc__': None, 'n_gpu': 1, 'local_size': 1, 'num_gpus': 1, 'distributed': True, 'num_nodes': 1, 'word_size': 1, 'local_rank': 0, 'rank': 0, 'node_id': 0, 'dist': True, 'nodes': 1, 'world_size': 1, 'train_batch_size': 32, 'eval_batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "from utils.args import sharedArgs\n",
    "\n",
    "manual_args = ['--cf', 'config/ref_attn_clip_combine_controlnet/app_demo_image_edit.py', '--eval_visu', 'True', '--root_dir', '/scratch/namrata/3d_pose_gen', '--local_train_batch_size', '32', '--local_eval_batch_size', '32', '--log_dir', 'exp/tiktok_ft5', '--epochs', '20', '--deepspeed', '--eval_step', '500',\n",
    "               '--save_step', '500', '--gradient_accumulate_steps', '1', '--learning_rate', '2e-4', '--fix_dist_seed', 'True', '--loss_target',\n",
    "               'noise', '--unet_unfreeze_type', 'all', '--guidance_scale', '3', '--refer_sdvae', 'True', '--ref_null_caption', 'False', '--combine_clip_local', 'True', '--combine_use_mask', 'True', '--conds', 'poses','masks', '--pretrained_model', 'exp/tiktok_ft22_final/1999.pth/mp_rank_00_model_states.pt', '--eval_save_filename', 'try']\n",
    "parsed_args = sharedArgs.parser.parse_args(args=manual_args)\n",
    "# args = sharedArgs.parser.parse_args(args=['--cf', 'config/ref_attn_clip_combine_controlnet/app_demo_image_edit.py', '--eval_visu', 'True'])\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "\n",
    "###### process the args #######\n",
    "if parsed_args.root_dir:\n",
    "    BasicArgs.root_dir = parsed_args.root_dir\n",
    "else:\n",
    "    parsed_args.root_dir = BasicArgs.root_dir\n",
    "parsed_args.pretrained_model_path = os.path.join(parsed_args.root_dir, parsed_args.pretrained_model_path)\n",
    "\n",
    "def parse_with_cf(parsed_args):\n",
    "    \"\"\"This function will set args based on the input config file.\n",
    "    (1) it only overwrites unset parameters,\n",
    "        i.e., these parameters not set from user command line input\n",
    "    (2) it also sets configs in the config file but declared in the parser\n",
    "    \"\"\"\n",
    "    # convert to EasyDict object,\n",
    "    # enabling access from attributes even for nested config\n",
    "    # e.g., args.train_datasets[0].name\n",
    "    args = edict(vars(parsed_args))\n",
    "    if os.path.exists(parsed_args.cf):\n",
    "        cf = import_filename(parsed_args.cf)\n",
    "        config_args = edict(vars(cf.Args))\n",
    "        override_keys = {arg[2:].split(\"=\")[0] for arg in manual_args\n",
    "                         if arg.startswith(\"--\")}\n",
    "        # import pdb;pdb.set_trace()\n",
    "        for k, v in config_args.items():\n",
    "            if k not in override_keys:\n",
    "                setattr(args, k, v)\n",
    "    else:\n",
    "        raise NotImplementedError('Config filename %s does not exist.' % args.cf)\n",
    "    return args\n",
    "\n",
    "args = parse_with_cf(parsed_args)\n",
    "        \n",
    "args.n_gpu = T.cuda.device_count() # local size\n",
    "args.local_size = args.n_gpu\n",
    "if args.root_dir not in args.log_dir:\n",
    "    args.log_dir = os.path.join(args.root_dir, args.log_dir)\n",
    "if args.stepwise_sample_depth == -1:\n",
    "    args.interpolation = None\n",
    "    args.interpolate_mode = None\n",
    "if args.interpolation != \"interpolate\":\n",
    "    args.interpolate_mode = None\n",
    "\n",
    "assert args.eval_step > 0, \"eval_step must be positive\"\n",
    "assert args.save_step > 0, \"save_step must be positive\"\n",
    "\n",
    "dist_init(args)\n",
    "args.dist = args.distributed\n",
    "args.nodes = args.num_nodes\n",
    "args.world_size = args.num_gpus\n",
    "args.train_batch_size = args.local_train_batch_size * args.world_size\n",
    "args.eval_batch_size = args.local_eval_batch_size * args.world_size\n",
    "#############################################\n",
    "\n",
    "cf = import_filename(args.cf)\n",
    "Net, inner_collect_fn = cf.Net, cf.inner_collect_fn\n",
    "\n",
    "dataset_cf = import_filename(args.dataset_cf)\n",
    "BaseDataset = dataset_cf.BaseDataset\n",
    "\n",
    "# args = update_args(parsed_args, args)\n",
    "\n",
    "# init models\n",
    "logger.info('Building models...')\n",
    "model = Net(args)\n",
    "print(f\"Args: {edict(vars(args))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e44c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a550cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f876d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1666198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2024-09-26 18:01:14 <287074974.py:3> <module>] Do eval_visu...\u001b[0m\n",
      "\u001b[33m[2024-09-26 18:01:14 <287074974.py:3> <module>] Do eval_visu...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 25\n",
      "DEEPSPEED config {'train_micro_batch_size_per_gpu': 32, 'gradient_accumulation_steps': 1, 'fp16': {'enabled': True, 'loss_scale': 64}, 'flops_profiler': {'enabled': False, 'profile_step': 1, 'module_depth': -1, 'top_modules': 3, 'detailed': True}, 'tensorboard': {'enabled': True, 'output_path': '/scratch/namrata/3d_pose_gen/exp/tiktok_ft5', 'job_name': 'tensorboard_log'}}\n",
      "Specify the load model path, not use deepspeed but the pytorch original load func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 18:01:21 <wutils_ldm.py:462> file2data] Loaded data from /scratch/namrata/3d_pose_gen/exp/tiktok_ft22_final/1999.pth/mp_rank_00_model_states.pt\u001b[0m\n",
      "[2024-09-26 18:01:21 <wutils_ldm.py:462> file2data] Loaded data from /scratch/namrata/3d_pose_gen/exp/tiktok_ft22_final/1999.pth/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[33m[2024-09-26 18:01:28 <wutils_ldm.py:707> adaptively_load_state_dict] Strictly Loaded state_dict.\u001b[0m\n",
      "\u001b[33m[2024-09-26 18:01:28 <wutils_ldm.py:707> adaptively_load_state_dict] Strictly Loaded state_dict.\u001b[0m\n",
      "\u001b[33m[2024-09-26 18:01:28 <agent.py:877> load_checkpoint_for_deepspeed_diff_gpu] Loaded checkpoint exp/tiktok_ft22_final/1999.pth/mp_rank_00_model_states.pt of global_step 1000\u001b[0m\n",
      "\u001b[33m[2024-09-26 18:01:28 <agent.py:877> load_checkpoint_for_deepspeed_diff_gpu] Loaded checkpoint exp/tiktok_ft22_final/1999.pth/mp_rank_00_model_states.pt of global_step 1000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-26 18:01:28,930] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.15.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-09-26 18:01:28,931] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-09-26 18:01:28,932] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1\n",
      "[2024-09-26 18:01:29,527] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-09-26 18:01:29,535] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2024-09-26 18:01:29,536] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-09-26 18:01:29,537] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2024-09-26 18:01:29,537] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2024-09-26 18:01:29,537] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2024-09-26 18:01:29,538] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-09-26 18:01:29,538] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2024-09-26 18:01:29,538] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-09-26 18:01:29,539] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-09-26 18:01:29,539] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-09-26 18:01:29,539] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-09-26 18:01:29,539] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f32c87457d0>\n",
      "[2024-09-26 18:01:29,540] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2024-09-26 18:01:29,540] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-09-26 18:01:29,540] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2024-09-26 18:01:29,541] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2024-09-26 18:01:29,541] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-09-26 18:01:29,541] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2024-09-26 18:01:29,541] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2024-09-26 18:01:29,542] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2024-09-26 18:01:29,542] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2024-09-26 18:01:29,542] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-09-26 18:01:29,542] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2024-09-26 18:01:29,543] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-09-26 18:01:29,543] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-09-26 18:01:29,543] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-09-26 18:01:29,544] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-09-26 18:01:29,544] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-09-26 18:01:29,544] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-09-26 18:01:29,544] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2024-09-26 18:01:29,545] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2024-09-26 18:01:29,545] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 3, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-09-26 18:01:29,545] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2024-09-26 18:01:29,546] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2024-09-26 18:01:29,546] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-09-26 18:01:29,547] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2024-09-26 18:01:29,551] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2024-09-26 18:01:29,551] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2024-09-26 18:01:29,552] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2024-09-26 18:01:29,552] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-09-26 18:01:29,553] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2024-09-26 18:01:29,553] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-09-26 18:01:29,553] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-09-26 18:01:29,554] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2024-09-26 18:01:29,554] [INFO] [config.py:1003:print]   loss_scale ................... 64\n",
      "[2024-09-26 18:01:29,555] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2024-09-26 18:01:29,555] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2024-09-26 18:01:29,556] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2024-09-26 18:01:29,556] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='/scratch/namrata/3d_pose_gen/exp/tiktok_ft5', job_name='tensorboard_log') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2024-09-26 18:01:29,557] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-09-26 18:01:29,557] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-09-26 18:01:29,557] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2024-09-26 18:01:29,558] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2024-09-26 18:01:29,558] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-09-26 18:01:29,559] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2024-09-26 18:01:29,559] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2024-09-26 18:01:29,559] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2024-09-26 18:01:29,560] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2024-09-26 18:01:29,560] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2024-09-26 18:01:29,560] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-09-26 18:01:29,561] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2024-09-26 18:01:29,561] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2024-09-26 18:01:29,562] [INFO] [config.py:1003:print]   steps_per_print .............. 10\n",
      "[2024-09-26 18:01:29,562] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2024-09-26 18:01:29,562] [INFO] [config.py:1003:print]   train_batch_size ............. 32\n",
      "[2024-09-26 18:01:29,563] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-09-26 18:01:29,563] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2024-09-26 18:01:29,563] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2024-09-26 18:01:29,563] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2024-09-26 18:01:29,564] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2024-09-26 18:01:29,564] [INFO] [config.py:1003:print]   world_size ................... 1\n",
      "[2024-09-26 18:01:29,564] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2024-09-26 18:01:29,565] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-09-26 18:01:29,565] [INFO] [config.py:1003:print]   zero_enabled ................. False\n",
      "[2024-09-26 18:01:29,565] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-09-26 18:01:29,565] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0\n",
      "[2024-09-26 18:01:29,566] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 64\n",
      "    }, \n",
      "    \"flops_profiler\": {\n",
      "        \"enabled\": false, \n",
      "        \"profile_step\": 1, \n",
      "        \"module_depth\": -1, \n",
      "        \"top_modules\": 3, \n",
      "        \"detailed\": true\n",
      "    }, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": true, \n",
      "        \"output_path\": \"/scratch/namrata/3d_pose_gen/exp/tiktok_ft5\", \n",
      "        \"job_name\": \"tensorboard_log\"\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-26 18:01:29 <agent.py:241> prepare_dist_model] Successfully built models with {'trainable': 1581790996, 'frozen': 387620071, 'trainable_fp32': 0, 'trainalbe_fp16': 1581790996, 'frozen_fp32': 303966208, 'frozen_fp16': 83653863} parameters\u001b[0m\n",
      "[2024-09-26 18:01:29 <agent.py:241> prepare_dist_model] Successfully built models with {'trainable': 1581790996, 'frozen': 387620071, 'trainable_fp32': 0, 'trainalbe_fp16': 1581790996, 'frozen_fp32': 303966208, 'frozen_fp16': 83653863} parameters\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    }
   ],
   "source": [
    "### prepare the eval\n",
    "\n",
    "logger.warning(\"Do eval_visu...\")\n",
    "if getattr(args, 'refer_clip_preprocess', None):\n",
    "    eval_dataset = BaseDataset(args, args.val_yaml, split='val', preprocesser=model.feature_extractor)\n",
    "else:\n",
    "    eval_dataset = BaseDataset(args, args.val_yaml, split='val')\n",
    "eval_dataloader, eval_info = make_data_loader(\n",
    "    args, args.local_eval_batch_size, \n",
    "    eval_dataset)\n",
    "\n",
    "\n",
    "trainer = Agent_LDM(args=args, model=model)\n",
    "trainer.eval_demo_pre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4377766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image):\n",
    "    if not image.mode == \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(reference_fg, fg_mask, ref_bg_image, bg_mask, skeleton_img, *args, **kwargs):\n",
    "    reference_fg = load_image(reference_fg)\n",
    "    fg_mask = load_image(fg_mask)\n",
    "    ref_bg_image = load_image(ref_bg_image)\n",
    "    bg_mask = load_image(bg_mask)\n",
    "    skeleton_img = load_image(skeleton_img)\n",
    "    \n",
    "    input_data = [reference_fg, fg_mask, ref_bg_image, bg_mask, skeleton_img]\n",
    "    output_image = trainer.eval_demo_run(input_data, eval_dataset=eval_dataset)\n",
    "    return output_image\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_masked(reference_fg, ref_bg_image, skeleton_img, *args, **kwargs):\n",
    "    reference_fg = load_image(reference_fg)\n",
    "    ref_bg_image = load_image(ref_bg_image)\n",
    "    skeleton_img = load_image(skeleton_img)\n",
    "\n",
    "    print(reference_fg,ref_bg_image,skeleton_img)\n",
    "    \n",
    "    input_data = [reference_fg, ref_bg_image, skeleton_img]\n",
    "    output_image = trainer.eval_demo_run_masked(input_data, eval_dataset=eval_dataset)\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580155a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a1454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c3b8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://36cfad975c622f4ac0.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://36cfad975c622f4ac0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C54F610> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E94ACE750> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E94ACE610>\n",
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C689610> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C689D10> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C6C2910>\n",
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C46EBD0> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C46E4D0> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C46F550>\n",
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C46FC10> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C46FDD0> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C46F0D0>\n",
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 17.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C6B4C10> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C6B5350> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C6C0710>\n",
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C54F550> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C6C2D50> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C6C2450>\n",
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 14.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "title = \"DisCo Demo (Video Demo Coming Soon!)\"\n",
    "description = \"\"\"<p style='text-align: center'> <a href='https://disco-dance.github.io/' target='_blank'>Project Page</a> | <a href='https://arxiv.org/pdf/2212.11270.pdf' target='_blank'>Paper</a> | <a href='https://github.com/microsoft/X-Decoder' target='_blank'>Github Repo</a> | <a href='https://youtu.be/wYp6vmyolqE' target='_blank'>Video</a> </p>\n",
    "<p>Skip the queue by duplicating this space and upgrading to GPU in settings</p>\n",
    "<a href=\"https://huggingface.co/spaces/xdecoder/Demo?duplicate=true\"><img src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # DisCo Demo (Video Demo Coming Soon!)\n",
    "    Start editing the human with the provided human foreground, background, and pose.\n",
    "    \n",
    "    Note that for self-uploaded images, TikTok-Style human images are preferred.\n",
    "    \n",
    "    [Project Page](https://disco-dance.github.io/) | [Github](https://github.com/Wangt-CN/DisCo)\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(min_width=400, scale=2):\n",
    "            input_fg = gr.Image(type='pil', label=\"Foreground Image\")\n",
    "            gr.Examples(examples=[\"./demo_data/fg/masked_images/00035.png\", \"./demo_data/fg/masked_images/00335.png\", \"./demo_data/fg/masked_images/00147.png\", \"./demo_data/fg/masked_images/00072.png\", \"./demo_data/fg/masked_images/00115.png\"], inputs=input_fg)\n",
    "\n",
    "            input_bg = gr.Image(type='pil', label=\"Background Image\")\n",
    "            gr.Examples(examples=[\"./demo_data/bg/masked_images/00035.png\", \"./demo_data/bg/masked_images/00335.png\", \"./demo_data/bg/masked_images/00147.png\", \"./demo_data/bg/masked_images/00072.png\", \"./demo_data/bg/masked_images/00115.png\"], inputs=input_bg)\n",
    "\n",
    "            input_pose = gr.Image(type='pil', label=\"Target Pose\", scale=1)\n",
    "            gr.Examples(examples=[\"./demo_data/pose_img/0049.png\", \"./demo_data/pose_img/0198.png\", \"./demo_data/pose_img/0213.png\", \"./demo_data/pose_img/0264.png\", \"./demo_data/pose_img/0144.png\", \"./demo_data/pose_img/0054.png\"], inputs=input_pose)\n",
    "\n",
    "            btn = gr.Button(\"Generate\")\n",
    "        \n",
    "        with gr.Column(min_width=150):\n",
    "            output_img = gr.Image(type='pil', label=\"Edited Human Image\")\n",
    "\n",
    "    btn.click(inference_masked, inputs=[input_fg, input_bg, input_pose], outputs=[output_img], concurrency_limit=2)\n",
    "    \n",
    "demo.launch(share=True, max_threads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141af2b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.95 GiB. GPU 0 has a total capacity of 79.15 GiB of which 2.27 GiB is free. Process 2294293 has 54.54 GiB memory in use. Process 2301360 has 2.61 GiB memory in use. Process 2314023 has 10.37 GiB memory in use. Including non-PyTorch memory, this process has 9.30 GiB memory in use. Of the allocated memory 8.40 GiB is allocated by PyTorch, and 83.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 1: Load the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexp/tiktok_ft22_final/1999.pth/mp_rank_00_model_states.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 2: Generate images (Assume model outputs images directly)\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/torch/serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[1;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/torch/serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/torch/serialization.py:1466\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1461\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1466\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1467\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1468\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1471\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/torch/serialization.py:414\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 414\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/torch/serialization.py:392\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    391\u001b[0m     device \u001b[38;5;241m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/torch/storage.py:187\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice, non_blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:  \u001b[38;5;66;03m# type: ignore[type-var, misc] # noqa: E704\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/sahil/anaconda3/envs/disco1/lib/python3.11/site-packages/torch/_utils.py:89\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_sparse\n\u001b[1;32m     88\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse storage is not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 89\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.95 GiB. GPU 0 has a total capacity of 79.15 GiB of which 2.27 GiB is free. Process 2294293 has 54.54 GiB memory in use. Process 2301360 has 2.61 GiB memory in use. Process 2314023 has 10.37 GiB memory in use. Including non-PyTorch memory, this process has 9.30 GiB memory in use. Of the allocated memory 8.40 GiB is allocated by PyTorch, and 83.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F3048797C90> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C489B90> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F2E8C48A790>\n",
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=256x256 at 0x7F30486C4C10> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F304847F7D0> <PIL.Image.Image image mode=RGB size=256x256 at 0x7F304847E110>\n",
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.31it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Load the model\n",
    "model = torch.load('exp/tiktok_ft22_final/1999.pth/mp_rank_00_model_states.pt')\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Step 2: Generate images (Assume model outputs images directly)\n",
    "def generate_images(model, num_images, device='cuda'):\n",
    "    model.to(device)\n",
    "    images = []\n",
    "    for _ in range(num_images):\n",
    "        # Assuming your model generates an image with a forward pass\n",
    "        with torch.no_grad():\n",
    "            img = model()  # Replace with actual model generation code\n",
    "            images.append(img.cpu())\n",
    "    return torch.stack(images)\n",
    "\n",
    "# Step 3: Use Inception to extract features\n",
    "def get_inception_features(images, batch_size=16, device='cuda'):\n",
    "    inception = models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception.eval()\n",
    "    \n",
    "    # Define the transform for input to Inception\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Process images in batches\n",
    "    loader = DataLoader(images, batch_size=batch_size, shuffle=False)\n",
    "    features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_batch in loader:\n",
    "            img_batch = img_batch.to(device)\n",
    "            feat = inception(img_batch).cpu().numpy()\n",
    "            features.append(feat)\n",
    "    \n",
    "    return np.concatenate(features)\n",
    "\n",
    "# Step 4: Calculate the FID Score\n",
    "def calculate_fid(mu1, sigma1, mu2, sigma2):\n",
    "    # Compute FID using mean and covariance\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # Handle imaginary numbers from sqrtm\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Step 5: Main function to calculate FID\n",
    "def fid_score(model, real_images, num_generated_images=1000, device='cuda'):\n",
    "    # Generate images from the model\n",
    "    generated_images = generate_images(model, num_generated_images, device)\n",
    "\n",
    "    # Extract Inception features for real and generated images\n",
    "    real_features = get_inception_features(real_images, device=device)\n",
    "    generated_features = get_inception_features(generated_images, device=device)\n",
    "\n",
    "    # Calculate mean and covariance of the features\n",
    "    mu_real, sigma_real = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu_gen, sigma_gen = np.mean(generated_features, axis=0), np.cov(generated_features, rowvar=False)\n",
    "\n",
    "    # Calculate FID score\n",
    "    fid = calculate_fid(mu_real, sigma_real, mu_gen, sigma_gen)\n",
    "    return fid\n",
    "\n",
    "# Example usage\n",
    "# Load real images dataset (this is an example, you should provide real images)\n",
    "real_images_dataset = ImageFolder(root='demo_data/fg/images', transform=transforms.ToTensor())\n",
    "real_images = DataLoader(real_images_dataset, batch_size=32)\n",
    "\n",
    "# Calculate FID score\n",
    "fid_value = fid_score(model, real_images)\n",
    "print(f'FID Score: {fid_value}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
