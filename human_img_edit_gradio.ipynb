{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473523df",
   "metadata": {},
   "source": [
    "### Example Jupyter Code for Generation Results with Gradio Demo\n",
    "\n",
    "```\n",
    "--------------------------------------------------------\n",
    "DisCo - Disentangled Control for Referring Human Dance Generation in Real World\n",
    "Licensed under The Apache-2.0 license License [see LICENSE for details]\n",
    "Tan Wang (TAN317@e.ntu.edu.sg)\n",
    "Work done during internship at Microsoft\n",
    "--------------------------------------------------------\n",
    "```\n",
    "\n",
    "Pls remember to change the path (model checkpoint, root dir, eval_save_filename, and so on) in the `manual_args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bea7bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbf0c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-02 04:42:28 <wutils_ldm.py:150> <module>] <utils.py>: Deep Learning Utils @ Chenfei Wu\u001b[0m\n",
      "/home/namrata/anaconda3/envs/disco1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WANDB_ENABLE: 0\n",
      "2024-09-02 04:42:28,712.712 27228:common.py:1785 setup_yaml(): python 3 env\n",
      "[2024-09-02 04:42:28,743] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namrata/anaconda3/envs/disco1/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/namrata/anaconda3/envs/disco1/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "[2024-09-02 04:42:29 <wutils.py:132> <module>] <utils.py>: Deep Learning Utils @ Chenfei Wu\u001b[0m\n",
      "[2024-09-02 04:42:29 <wutils.py:132> <module>] <utils.py>: Deep Learning Utils @ Chenfei Wu\u001b[0m\n",
      "[2024-09-02 04:42:29 <__init__.py:71> BasicArgs] Detected unknown Node namratapc.\u001b[0m\n",
      "[2024-09-02 04:42:29 <__init__.py:71> BasicArgs] Detected unknown Node namratapc.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_ENABLE\"] = \"0\"\n",
    "\n",
    "from utils.wutils_ldm import *\n",
    "from agent import Agent_LDM, WarmupLinearLR, WarmupLinearConstantLR\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from config import BasicArgs\n",
    "from utils.lib import *\n",
    "from utils.args import parse_with_cf\n",
    "\n",
    "from utils.dist import dist_init\n",
    "\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '23456'\n",
    "os.environ[\"USE_LIBUV\"] = \"0\"\n",
    "\n",
    "# # initialize the process group\n",
    "# dist.init_process_group(\n",
    "#     backend=\"cuda:gloo\",\n",
    "#     rank=0,\n",
    "#     world_size=1,\n",
    "#     init_method=f\"tcp://localhost:23456\",\n",
    "# )\n",
    "\n",
    "from dataset.tsv_dataset import make_data_sampler, make_batch_data_sampler\n",
    "from finetune_sdm_yaml import get_loader_info, make_data_loader\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8735d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fcd501",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-02 04:42:30 <3177181451.py:72> <module>] Building models...\u001b[0m\n",
      "[2024-09-02 04:42:30 <3177181451.py:72> <module>] Building models...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed training ... presumbly debug with 1 GPU\n",
      "Using seed 42 for rank 0\n",
      "Using seed 42 for torch.cuda\n",
      "Loading pre-trained image_encoder from /media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/diffusers/sd-image-variations-diffusers/image_encoder\n",
      "Loading pre-trained vae from /media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/diffusers/sd-image-variations-diffusers/vae\n",
      "Loading pre-trained unet from /media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/diffusers/sd-image-variations-diffusers/unet\n",
      "use sd vae to init the controlnet condition embedding\n",
      "Args: {'root_dir': '/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/', 'cf': 'config/ref_attn_clip_combine_controlnet/app_demo_image_edit.py', 'pretrained_model': '/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/dataset/disco/ft_checkpoint/moretiktok_nocfg/mp_rank_00_model_states.pt', 'pretrained_model_lora': None, 'pretrained_model_controlnet': None, 'debug': False, 'debug_seed': False, 'debug_dataloader': False, 'log_dir': '/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/exp/tiktok_ft', 'deepspeed': True, 'use_amp': False, 'seed': 42, 'fix_dist_seed': True, 'tiktok_data_root': 'keli/dataset/TikTok_dataset/', 'img_size': (256, 256), 'max_video_len': 1, 'debug_max_video_len': 1, 'conds': ['poses', 'masks'], 'gradient_checkpointing': True, 'find_unused_parameters': False, 'enable_xformers_memory_efficient_attention': True, 'trainable_modules': None, 'scale_factor': 0.18215, 'loss_target': 'noise', 'x0_steps': 200, 'pretrained_model_path': '/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/diffusers/sd-image-variations-diffusers', 'num_workers': 4, 'node_split_sampler': False, 'gradient_accumulate_steps': 1, 'max_grad_norm': -1, 'learning_rate': 0.0002, 'decay': 0.001, 'warmup_ratio': 0.1, 'max_train_samples': None, 'debug_max_train_samples': 100, 'drop_text': 1.0, 'local_train_batch_size': 32, 'epochs': 20, 'eval_step': 500.0, 'save_step': 500.0, 'do_train': False, 'train_yaml': './blob_dir/debug_output/video_sythesis/dataset/composite/train_webvid10m_a_54.yaml', 'resume': False, 'null_caption': False, 'refer_sdvae': True, 'controlnet_conditioning_scale_cond': 1.0, 'controlnet_conditioning_scale_ref': 1.0, 'nframes': 8, 'frame_interval': 1, 'eval_sample_interval': 1, 'train_sample_interval': 1, 'unet_unfreeze_type': 'all', 'controlnet_attn': False, 'use_cfg': False, 'refer_clip_preprocess': False, 'refer_clip_proj': False, 'ref_null_caption': False, 'combine_clip_local': True, 'combine_use_mask': True, 'drop_ref': 0.0, 'my_adapter': False, 'pos_resize_img': False, 'fg_variation': 0.0, 'strong_aug_stage2': False, 'strong_rand_stage2': False, 'strong_aug_stage1': False, 'stage1_pretrain_path': None, 'stage2_only_pose': False, 'constant_lr': False, 'SD2_not_add_image_emb_noise': False, 'val_yaml': './blob_dir/debug_output/video_sythesis/dataset/composite/val_webvid10m_a.yaml', 'max_eval_samples': None, 'debug_max_eval_samples': 20, 'pose_normalize': False, 'normalize_by_1st_frm': False, 'local_eval_batch_size': 32, 'eval_visu': True, 'eval_visu_trainsample': False, 'eval_visu_imagefolder': False, 'eval_visu_changepose': False, 'eval_visu_changefore': False, 'eval_save_filename': 'try', 'eval_before_train': True, 'eval_scheduler': 'ddim', 'eval_enc_dec_only': False, 'num_inf_videos_per_prompt': 1, 'num_inference_steps': 50, 'guidance_scale': 3.0, 'stepwise_sample_depth': -1, 'interpolation': None, 'interpolate_mode': None, 'visu_save': False, 'freeze_pose': False, 'freeze_background': False, 'ft_img_num': 0, 'ft_one_ref_image': True, 'ft_iters': None, 's1': 1.0, 's2': 1.0, 'ft_idx': None, 'ref_mode': 'first', '__module__': 'mymodule', 'task_name': 'ref_attn_clip_combine_controlnet', 'method_name': 'app_demo_image_edit', 'dataset_cf': 'dataset/app_demo_human_image_edit_singleinput.py', 'img_full_size': (256, 256), 'fps': 5, 'data_dir': './blob_dir/data/mtp_vlp_ray/debug/debug_pretrain', 'debug_train_yaml': './blob_dir/data/mtp_vlp_ray/debug/debug_pretrain/composite/train_webvid2.5m_2.yaml', 'debug_val_yaml': './blob_dir/data/mtp_vlp_ray/debug/debug_pretrain/composite/val_webvid2.5m.yaml', 'web_data_root': '/datadrive_d/wangtan/azure_storage/vigstandard_data/linjli/debug_output/video_sythesis/dataset/Lindsey_0504_youtube/frames/single_person', 'sd15_path': '/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/diffusers/stable-diffusion-v1-5-2', 'freeze_unet': True, 'num_inf_images_per_prompt': 1, '__doc__': None, 'n_gpu': 1, 'local_size': 1, 'num_gpus': 1, 'distributed': True, 'num_nodes': 1, 'word_size': 1, 'local_rank': 0, 'rank': 0, 'node_id': 0, 'dist': True, 'nodes': 1, 'world_size': 1, 'train_batch_size': 32, 'eval_batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "from utils.args import sharedArgs\n",
    "\n",
    "manual_args = ['--cf', 'config/ref_attn_clip_combine_controlnet/app_demo_image_edit.py', '--eval_visu', 'True', '--root_dir', '/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/', '--local_train_batch_size', '32', '--local_eval_batch_size', '32', '--log_dir', 'exp/tiktok_ft', '--epochs', '20', '--deepspeed', '--eval_step', '500',\n",
    "               '--save_step', '500', '--gradient_accumulate_steps', '1', '--learning_rate', '2e-4', '--fix_dist_seed', 'True', '--loss_target',\n",
    "               'noise', '--unet_unfreeze_type', 'all', '--guidance_scale', '3', '--refer_sdvae', 'True', '--ref_null_caption', 'False', '--combine_clip_local', 'True', '--combine_use_mask', 'True', '--conds', 'poses','masks', '--pretrained_model', '/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/dataset/disco/ft_checkpoint/moretiktok_nocfg/mp_rank_00_model_states.pt', '--eval_save_filename', 'try']\n",
    "parsed_args = sharedArgs.parser.parse_args(args=manual_args)\n",
    "# args = sharedArgs.parser.parse_args(args=['--cf', 'config/ref_attn_clip_combine_controlnet/app_demo_image_edit.py', '--eval_visu', 'True'])\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "###### process the args #######\n",
    "if parsed_args.root_dir:\n",
    "    BasicArgs.root_dir = parsed_args.root_dir\n",
    "else:\n",
    "    parsed_args.root_dir = BasicArgs.root_dir\n",
    "parsed_args.pretrained_model_path = os.path.join(parsed_args.root_dir, parsed_args.pretrained_model_path)\n",
    "\n",
    "def parse_with_cf(parsed_args):\n",
    "    \"\"\"This function will set args based on the input config file.\n",
    "    (1) it only overwrites unset parameters,\n",
    "        i.e., these parameters not set from user command line input\n",
    "    (2) it also sets configs in the config file but declared in the parser\n",
    "    \"\"\"\n",
    "    # convert to EasyDict object,\n",
    "    # enabling access from attributes even for nested config\n",
    "    # e.g., args.train_datasets[0].name\n",
    "    args = edict(vars(parsed_args))\n",
    "    if os.path.exists(parsed_args.cf):\n",
    "        cf = import_filename(parsed_args.cf)\n",
    "        config_args = edict(vars(cf.Args))\n",
    "        override_keys = {arg[2:].split(\"=\")[0] for arg in manual_args\n",
    "                         if arg.startswith(\"--\")}\n",
    "        # import pdb;pdb.set_trace()\n",
    "        for k, v in config_args.items():\n",
    "            if k not in override_keys:\n",
    "                setattr(args, k, v)\n",
    "    else:\n",
    "        raise NotImplementedError('Config filename %s does not exist.' % args.cf)\n",
    "    return args\n",
    "\n",
    "args = parse_with_cf(parsed_args)\n",
    "        \n",
    "args.n_gpu = T.cuda.device_count() # local size\n",
    "args.local_size = args.n_gpu\n",
    "if args.root_dir not in args.log_dir:\n",
    "    args.log_dir = os.path.join(args.root_dir, args.log_dir)\n",
    "if args.stepwise_sample_depth == -1:\n",
    "    args.interpolation = None\n",
    "    args.interpolate_mode = None\n",
    "if args.interpolation != \"interpolate\":\n",
    "    args.interpolate_mode = None\n",
    "\n",
    "assert args.eval_step > 0, \"eval_step must be positive\"\n",
    "assert args.save_step > 0, \"save_step must be positive\"\n",
    "\n",
    "dist_init(args)\n",
    "args.dist = args.distributed\n",
    "args.nodes = args.num_nodes\n",
    "args.world_size = args.num_gpus\n",
    "args.train_batch_size = args.local_train_batch_size * args.world_size\n",
    "args.eval_batch_size = args.local_eval_batch_size * args.world_size\n",
    "#############################################\n",
    "\n",
    "cf = import_filename(args.cf)\n",
    "Net, inner_collect_fn = cf.Net, cf.inner_collect_fn\n",
    "\n",
    "dataset_cf = import_filename(args.dataset_cf)\n",
    "BaseDataset = dataset_cf.BaseDataset\n",
    "\n",
    "# args = update_args(parsed_args, args)\n",
    "\n",
    "# init models\n",
    "logger.info('Building models...')\n",
    "model = Net(args)\n",
    "print(f\"Args: {edict(vars(args))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e44c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a550cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f876d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1666198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2024-09-02 04:42:36 <287074974.py:3> <module>] Do eval_visu...\u001b[0m\n",
      "\u001b[33m[2024-09-02 04:42:36 <287074974.py:3> <module>] Do eval_visu...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 25\n",
      "Specify the load model path, not use deepspeed but the pytorch original load func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-02 04:42:40 <wutils_ldm.py:456> file2data] Loaded data from /media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/dataset/disco/ft_checkpoint/moretiktok_nocfg/mp_rank_00_model_states.pt\u001b[0m\n",
      "[2024-09-02 04:42:40 <wutils_ldm.py:456> file2data] Loaded data from /media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/dataset/disco/ft_checkpoint/moretiktok_nocfg/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[33m[2024-09-02 04:42:41 <wutils_ldm.py:689> adaptively_load_state_dict] Some weights of state_dict were not used in target: ['vae.encoder.mid_block.attentions.0.query.weight', 'vae.encoder.mid_block.attentions.0.query.bias', 'vae.encoder.mid_block.attentions.0.key.weight', 'vae.encoder.mid_block.attentions.0.key.bias', 'vae.encoder.mid_block.attentions.0.value.weight', 'vae.encoder.mid_block.attentions.0.value.bias', 'vae.encoder.mid_block.attentions.0.proj_attn.weight', 'vae.encoder.mid_block.attentions.0.proj_attn.bias', 'vae.decoder.mid_block.attentions.0.query.weight', 'vae.decoder.mid_block.attentions.0.query.bias', 'vae.decoder.mid_block.attentions.0.key.weight', 'vae.decoder.mid_block.attentions.0.key.bias', 'vae.decoder.mid_block.attentions.0.value.weight', 'vae.decoder.mid_block.attentions.0.value.bias', 'vae.decoder.mid_block.attentions.0.proj_attn.weight', 'vae.decoder.mid_block.attentions.0.proj_attn.bias']\u001b[0m\n",
      "\u001b[33m[2024-09-02 04:42:41 <wutils_ldm.py:689> adaptively_load_state_dict] Some weights of state_dict were not used in target: ['vae.encoder.mid_block.attentions.0.query.weight', 'vae.encoder.mid_block.attentions.0.query.bias', 'vae.encoder.mid_block.attentions.0.key.weight', 'vae.encoder.mid_block.attentions.0.key.bias', 'vae.encoder.mid_block.attentions.0.value.weight', 'vae.encoder.mid_block.attentions.0.value.bias', 'vae.encoder.mid_block.attentions.0.proj_attn.weight', 'vae.encoder.mid_block.attentions.0.proj_attn.bias', 'vae.decoder.mid_block.attentions.0.query.weight', 'vae.decoder.mid_block.attentions.0.query.bias', 'vae.decoder.mid_block.attentions.0.key.weight', 'vae.decoder.mid_block.attentions.0.key.bias', 'vae.decoder.mid_block.attentions.0.value.weight', 'vae.decoder.mid_block.attentions.0.value.bias', 'vae.decoder.mid_block.attentions.0.proj_attn.weight', 'vae.decoder.mid_block.attentions.0.proj_attn.bias']\u001b[0m\n",
      "\u001b[33m[2024-09-02 04:42:41 <wutils_ldm.py:697> adaptively_load_state_dict] Some weights of target are missing in state_dict: ['vae.encoder.mid_block.attentions.0.to_q.weight', 'vae.encoder.mid_block.attentions.0.to_q.bias', 'vae.encoder.mid_block.attentions.0.to_k.weight', 'vae.encoder.mid_block.attentions.0.to_k.bias', 'vae.encoder.mid_block.attentions.0.to_v.weight', 'vae.encoder.mid_block.attentions.0.to_v.bias', 'vae.encoder.mid_block.attentions.0.to_out.0.weight', 'vae.encoder.mid_block.attentions.0.to_out.0.bias', 'vae.decoder.mid_block.attentions.0.to_q.weight', 'vae.decoder.mid_block.attentions.0.to_q.bias', 'vae.decoder.mid_block.attentions.0.to_k.weight', 'vae.decoder.mid_block.attentions.0.to_k.bias', 'vae.decoder.mid_block.attentions.0.to_v.weight', 'vae.decoder.mid_block.attentions.0.to_v.bias', 'vae.decoder.mid_block.attentions.0.to_out.0.weight', 'vae.decoder.mid_block.attentions.0.to_out.0.bias']\u001b[0m\n",
      "\u001b[33m[2024-09-02 04:42:41 <wutils_ldm.py:697> adaptively_load_state_dict] Some weights of target are missing in state_dict: ['vae.encoder.mid_block.attentions.0.to_q.weight', 'vae.encoder.mid_block.attentions.0.to_q.bias', 'vae.encoder.mid_block.attentions.0.to_k.weight', 'vae.encoder.mid_block.attentions.0.to_k.bias', 'vae.encoder.mid_block.attentions.0.to_v.weight', 'vae.encoder.mid_block.attentions.0.to_v.bias', 'vae.encoder.mid_block.attentions.0.to_out.0.weight', 'vae.encoder.mid_block.attentions.0.to_out.0.bias', 'vae.decoder.mid_block.attentions.0.to_q.weight', 'vae.decoder.mid_block.attentions.0.to_q.bias', 'vae.decoder.mid_block.attentions.0.to_k.weight', 'vae.decoder.mid_block.attentions.0.to_k.bias', 'vae.decoder.mid_block.attentions.0.to_v.weight', 'vae.decoder.mid_block.attentions.0.to_v.bias', 'vae.decoder.mid_block.attentions.0.to_out.0.weight', 'vae.decoder.mid_block.attentions.0.to_out.0.bias']\u001b[0m\n",
      "\u001b[33m[2024-09-02 04:42:41 <agent.py:847> load_checkpoint_for_deepspeed_diff_gpu] Loaded checkpoint /media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/dataset/disco/ft_checkpoint/moretiktok_nocfg/mp_rank_00_model_states.pt of global_step 7200\u001b[0m\n",
      "\u001b[33m[2024-09-02 04:42:41 <agent.py:847> load_checkpoint_for_deepspeed_diff_gpu] Loaded checkpoint /media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/dataset/disco/ft_checkpoint/moretiktok_nocfg/mp_rank_00_model_states.pt of global_step 7200\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-02 04:42:41,620] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.15.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-09-02 04:42:41,621] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-09-02 04:42:41,621] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1\n",
      "[2024-09-02 04:42:41,995] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-09-02 04:42:42,001] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2024-09-02 04:42:42,002] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-09-02 04:42:42,002] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2024-09-02 04:42:42,003] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2024-09-02 04:42:42,003] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2024-09-02 04:42:42,004] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-09-02 04:42:42,004] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2024-09-02 04:42:42,004] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-09-02 04:42:42,005] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-09-02 04:42:42,005] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-09-02 04:42:42,005] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-09-02 04:42:42,005] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x76a61b7ca4c0>\n",
      "[2024-09-02 04:42:42,006] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2024-09-02 04:42:42,006] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-09-02 04:42:42,006] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2024-09-02 04:42:42,007] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2024-09-02 04:42:42,007] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-09-02 04:42:42,007] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2024-09-02 04:42:42,007] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2024-09-02 04:42:42,008] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2024-09-02 04:42:42,008] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2024-09-02 04:42:42,008] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-09-02 04:42:42,008] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2024-09-02 04:42:42,009] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-09-02 04:42:42,009] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-09-02 04:42:42,009] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-09-02 04:42:42,009] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-09-02 04:42:42,009] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-09-02 04:42:42,010] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-09-02 04:42:42,010] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2024-09-02 04:42:42,010] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2024-09-02 04:42:42,011] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 3, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-09-02 04:42:42,011] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2024-09-02 04:42:42,011] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2024-09-02 04:42:42,012] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-09-02 04:42:42,012] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2024-09-02 04:42:42,012] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2024-09-02 04:42:42,013] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2024-09-02 04:42:42,013] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2024-09-02 04:42:42,013] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-09-02 04:42:42,013] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2024-09-02 04:42:42,014] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-09-02 04:42:42,014] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-09-02 04:42:42,015] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2024-09-02 04:42:42,015] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2024-09-02 04:42:42,016] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2024-09-02 04:42:42,016] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2024-09-02 04:42:42,016] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2024-09-02 04:42:42,017] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=True, output_path='/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/exp/tiktok_ft', job_name='tensorboard_log') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2024-09-02 04:42:42,017] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-09-02 04:42:42,017] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-09-02 04:42:42,017] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2024-09-02 04:42:42,017] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2024-09-02 04:42:42,018] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-09-02 04:42:42,018] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2024-09-02 04:42:42,018] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2024-09-02 04:42:42,019] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2024-09-02 04:42:42,019] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2024-09-02 04:42:42,019] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2024-09-02 04:42:42,019] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-09-02 04:42:42,020] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2024-09-02 04:42:42,020] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2024-09-02 04:42:42,020] [INFO] [config.py:1003:print]   steps_per_print .............. 10\n",
      "[2024-09-02 04:42:42,020] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2024-09-02 04:42:42,022] [INFO] [config.py:1003:print]   train_batch_size ............. 32\n",
      "[2024-09-02 04:42:42,022] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  32\n",
      "[2024-09-02 04:42:42,022] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2024-09-02 04:42:42,022] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2024-09-02 04:42:42,022] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2024-09-02 04:42:42,023] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2024-09-02 04:42:42,023] [INFO] [config.py:1003:print]   world_size ................... 1\n",
      "[2024-09-02 04:42:42,023] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2024-09-02 04:42:42,023] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-09-02 04:42:42,024] [INFO] [config.py:1003:print]   zero_enabled ................. False\n",
      "[2024-09-02 04:42:42,024] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-09-02 04:42:42,024] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0\n",
      "[2024-09-02 04:42:42,024] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 32, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"flops_profiler\": {\n",
      "        \"enabled\": false, \n",
      "        \"profile_step\": 1, \n",
      "        \"module_depth\": -1, \n",
      "        \"top_modules\": 3, \n",
      "        \"detailed\": true\n",
      "    }, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": true, \n",
      "        \"output_path\": \"/media/namrata/NAMRATA/Yeshiva/Capstone/DisCo/exp/tiktok_ft\", \n",
      "        \"job_name\": \"tensorboard_log\"\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-02 04:42:42 <agent.py:237> prepare_dist_model] Successfully built models with {'trainable': 1581790996, 'frozen': 387620071, 'trainable_fp32': 0, 'trainalbe_fp16': 1581790996, 'frozen_fp32': 303966208, 'frozen_fp16': 83653863} parameters\u001b[0m\n",
      "[2024-09-02 04:42:42 <agent.py:237> prepare_dist_model] Successfully built models with {'trainable': 1581790996, 'frozen': 387620071, 'trainable_fp32': 0, 'trainalbe_fp16': 1581790996, 'frozen_fp32': 303966208, 'frozen_fp16': 83653863} parameters\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    }
   ],
   "source": [
    "### prepare the eval\n",
    "\n",
    "logger.warning(\"Do eval_visu...\")\n",
    "if getattr(args, 'refer_clip_preprocess', None):\n",
    "    eval_dataset = BaseDataset(args, args.val_yaml, split='val', preprocesser=model.feature_extractor)\n",
    "else:\n",
    "    eval_dataset = BaseDataset(args, args.val_yaml, split='val')\n",
    "eval_dataloader, eval_info = make_data_loader(\n",
    "    args, args.local_eval_batch_size, \n",
    "    eval_dataset)\n",
    "\n",
    "\n",
    "trainer = Agent_LDM(args=args, model=model)\n",
    "trainer.eval_demo_pre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4377766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image):\n",
    "    if not image.mode == \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(reference_fg, fg_mask, ref_bg_image, bg_mask, skeleton_img, *args, **kwargs):\n",
    "    reference_fg = load_image(reference_fg)\n",
    "    fg_mask = load_image(fg_mask)\n",
    "    ref_bg_image = load_image(ref_bg_image)\n",
    "    bg_mask = load_image(bg_mask)\n",
    "    skeleton_img = load_image(skeleton_img)\n",
    "    \n",
    "    input_data = [reference_fg, fg_mask, ref_bg_image, bg_mask, skeleton_img]\n",
    "    output_image = trainer.eval_demo_run(input_data, eval_dataset=eval_dataset)\n",
    "    return output_image\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_masked(reference_fg, ref_bg_image, skeleton_img, *args, **kwargs):\n",
    "    reference_fg = load_image(reference_fg)\n",
    "    ref_bg_image = load_image(ref_bg_image)\n",
    "    skeleton_img = load_image(skeleton_img)\n",
    "    \n",
    "    input_data = [reference_fg, ref_bg_image, skeleton_img]\n",
    "    output_image = trainer.eval_demo_run_masked(input_data, eval_dataset=eval_dataset)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580155a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a1454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c3b8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://2e41b8c5904473c0b0.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2e41b8c5904473c0b0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode [all]: There are 686 modules in unet to be set as requires_grad=True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 15.23it/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "title = \"DisCo Demo (Video Demo Coming Soon!)\"\n",
    "description = \"\"\"<p style='text-align: center'> <a href='https://disco-dance.github.io/' target='_blank'>Project Page</a> | <a href='https://arxiv.org/pdf/2212.11270.pdf' target='_blank'>Paper</a> | <a href='https://github.com/microsoft/X-Decoder' target='_blank'>Github Repo</a> | <a href='https://youtu.be/wYp6vmyolqE' target='_blank'>Video</a> </p>\n",
    "<p>Skip the queue by duplicating this space and upgrading to GPU in settings</p>\n",
    "<a href=\"https://huggingface.co/spaces/xdecoder/Demo?duplicate=true\"><img src=\"https://bit.ly/3gLdBN6\" alt=\"Duplicate Space\"></a>\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # DisCo Demo (Video Demo Coming Soon!)\n",
    "    Start editing the human with the provided human foreground, background, and pose.\n",
    "    \n",
    "    Note that for self-uploaded images, TikTok-Style human images are preferred.\n",
    "    \n",
    "    [Project Page](https://disco-dance.github.io/) | [Github](https://github.com/Wangt-CN/DisCo)\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(min_width=400, scale=2):\n",
    "            input_fg = gr.Image(type='pil', label=\"Foreground Image\")\n",
    "            gr.Examples(examples=[\"./demo_data/fg/masked_images/00035.png\", \"./demo_data/fg/masked_images/00335.png\", \"./demo_data/fg/masked_images/00147.png\", \"./demo_data/fg/masked_images/00072.png\", \"./demo_data/fg/masked_images/00115.png\"], inputs=input_fg)\n",
    "\n",
    "            input_bg = gr.Image(type='pil', label=\"Background Image\")\n",
    "            gr.Examples(examples=[\"./demo_data/bg/masked_images/00035.png\", \"./demo_data/bg/masked_images/00335.png\", \"./demo_data/bg/masked_images/00147.png\", \"./demo_data/bg/masked_images/00072.png\", \"./demo_data/bg/masked_images/00115.png\"], inputs=input_bg)\n",
    "\n",
    "            input_pose = gr.Image(type='pil', label=\"Target Pose\", scale=1)\n",
    "            gr.Examples(examples=[\"./demo_data/pose_img/0049.png\", \"./demo_data/pose_img/0198.png\", \"./demo_data/pose_img/0213.png\", \"./demo_data/pose_img/0264.png\", \"./demo_data/pose_img/0144.png\", \"./demo_data/pose_img/0054.png\"], inputs=input_pose)\n",
    "\n",
    "            btn = gr.Button(\"Generate\")\n",
    "        \n",
    "        with gr.Column(min_width=150):\n",
    "            output_img = gr.Image(type='pil', label=\"Edited Human Image\")\n",
    "\n",
    "    btn.click(inference_masked, inputs=[input_fg, input_bg, input_pose], outputs=[output_img], concurrency_limit=2)\n",
    "    \n",
    "demo.launch(share=True, max_threads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141af2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
